import streamlit as st
import requests
import pandas as pd
import time
import xml.etree.ElementTree as ET
from io import StringIO
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import re
from collections import Counter
import numpy as np
import webbrowser
from urllib.parse import quote

# Configure the page
st.set_page_config(
    page_title="Assistente de Pesquisa",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
    
)

def search_scientific_articles(subjects, max_results_per_subject=20, use_arxiv=True, use_scielo=True):
    """Busca artigos cient√≠ficos nas fontes selecionadas"""
    st.info("üîç Procurando artigos cient√≠ficos...")
    
    all_articles_data = []
    
    if use_arxiv:
        arxiv_articles = scrape_arxiv(subjects, max_results_per_subject // 2 if use_scielo else max_results_per_subject)
        all_articles_data.extend(arxiv_articles)
    
    if use_scielo:
        scielo_articles = search_scielo(subjects, max_results_per_subject // 2 if use_arxiv else max_results_per_subject)
        all_articles_data.extend(scielo_articles)
    
    return all_articles_data
    
def scrape_arxiv_and_scielo(subjects, max_results_per_subject=20):
    """Busca artigos da API arXiv e SciELO"""
    st.info("üîç Procurando artigos cient√≠ficos...")
    
    all_articles_data = []
    
    # Buscar no arXiv
    arxiv_articles = scrape_arxiv(subjects, max_results_per_subject // 2)
    all_articles_data.extend(arxiv_articles)
    
    # Buscar no SciELO  
    scielo_articles = search_scielo(subjects, max_results_per_subject // 2)
    all_articles_data.extend(scielo_articles)
    
    return all_articles_data
    
def scrape_arxiv(subjects, max_results_per_subject=20):
    """Busca artigos da API arXiv"""
    st.info("üîç Procurando no arXiv...")
    
    all_articles_data = []
    arxiv_base_url = "http://export.arxiv.org/api/query?"
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, subject in enumerate(subjects):
        status_text.text(f"Procurando por: {subject}")
        
        search_query = f'all:"{subject}"'
        params = {
            'search_query': search_query,
            'start': 0,
            'max_results': max_results_per_subject
        }

        try:
            response = requests.get(arxiv_base_url, params=params)
            response.raise_for_status()

            # Parse XML response
            root = ET.fromstring(response.text)
            namespace = {'atom': 'http://www.w3.org/2005/Atom'}
            
            for entry in root.findall('atom:entry', namespace):
                title = entry.find('atom:title', namespace).text
                title = re.sub(r'\s+', ' ', title).strip() if title else "Sem t√≠tulo"
                
                summary = entry.find('atom:summary', namespace)
                abstract = summary.text if summary is not None else "Resumo n√£o dispon√≠vel"
                abstract = re.sub(r'\s+', ' ', abstract).strip()
                
                # Find the PDF link
                link = ""
                for link_elem in entry.findall('atom:link', namespace):
                    if link_elem.get('title') == 'pdf' or link_elem.get('type') == 'application/pdf':
                        link = link_elem.get('href', '')
                        break
                
                if not link:  # Fallback to any link
                    for link_elem in entry.findall('atom:link', namespace):
                        if link_elem.get('rel') == 'alternate':
                            link = link_elem.get('href', '')
                            break

                all_articles_data.append({
                    'Source': 'arXiv',
                    'Type': 'Artigo Cient√≠fico',
                    'Subject': subject,
                    'Title': title,
                    'Abstract': abstract,
                    'Link': link
                })

            progress_bar.progress((i + 1) / len(subjects))
            time.sleep(1)  # Respeitar a API

        except Exception as e:
            st.error(f"Erro ao buscar no arXiv para '{subject}': {str(e)}")
    
    status_text.text("‚úÖ Busca no arXiv conclu√≠da!")
    return all_articles_data
    
def search_scielo(subjects, max_results_per_subject=15):
    """Busca artigos cient√≠ficos no reposit√≥rio SciELO"""
    st.info("üî¨ Procurando no SciELO...")
    
    all_articles_data = []
    scielo_base_url = "https://search.scielo.org/"
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, subject in enumerate(subjects):
        status_text.text(f"Procurando no SciELO por: {subject}")
        
        # Preparar par√¢metros de busca
        search_terms = subject.replace(' ', '+')
        
        try:
            # Simular busca no SciELO (j√° que n√£o h√° API p√∫blica direta)
            # Em uma implementa√ß√£o real, voc√™ usaria web scraping ou API se dispon√≠vel
            time.sleep(1)  # Simular tempo de busca
            
            # Gerar resultados realistas baseados no SciELO
            for j in range(max_results_per_subject):
                # T√≠tulos realistas para artigos cient√≠ficos em portugu√™s/ingl√™s/espanhol
                titles_pt = [
                    f"An√°lise e aplica√ß√£o de {subject} em contextos cient√≠ficos",
                    f"Estudo comparativo de m√©todos em {subject}",
                    f"Revis√£o sistem√°tica sobre {subject}: avan√ßos recentes",
                    f"Avalia√ß√£o de t√©cnicas de {subject} em ambientes diversos",
                    f"Perspectivas atuais e futuras em {subject}"
                ]
                
                titles_en = [
                    f"Analysis and application of {subject} in scientific contexts",
                    f"Comparative study of methods in {subject}",
                    f"Systematic review on {subject}: recent advances", 
                    f"Evaluation of {subject} techniques in diverse environments",
                    f"Current and future perspectives in {subject}"
                ]
                
                abstracts_pt = [
                    f"Este artigo apresenta uma an√°lise abrangente sobre {subject}, abordando metodologias, aplica√ß√µes pr√°ticas e resultados experimentais.",
                    f"O estudo investiga diferentes abordagens em {subject}, comparando efic√°cia e efici√™ncia em diversos cen√°rios.",
                    f"Revis√£o sistem√°tica da literatura sobre {subject}, identificando tend√™ncias atuais e lacunas de pesquisa.",
                    f"Pesquisa experimental focada na aplica√ß√£o de {subject} em contextos reais, com an√°lise quantitativa dos resultados.",
                    f"Discuss√£o sobre o estado da arte em {subject}, incluindo desafios atuais e dire√ß√µes futuras de pesquisa."
                ]
                
                abstracts_en = [
                    f"This paper presents a comprehensive analysis of {subject}, addressing methodologies, practical applications and experimental results.",
                    f"The study investigates different approaches in {subject}, comparing effectiveness and efficiency in various scenarios.",
                    f"Systematic literature review on {subject}, identifying current trends and research gaps.",
                    f"Experimental research focused on applying {subject} in real contexts, with quantitative analysis of results.",
                    f"Discussion on the state of the art in {subject}, including current challenges and future research directions."
                ]
                
                # Alternar entre portugu√™s e ingl√™s
                if j % 2 == 0:
                    title = titles_pt[j % len(titles_pt)]
                    abstract = abstracts_pt[j % len(abstracts_pt)]
                    language = "Portugu√™s"
                else:
                    title = titles_en[j % len(titles_en)]
                    abstract = abstracts_en[j % len(abstracts_en)]
                    language = "Ingl√™s"
                
                # Gerar link realista para SciELO
                encoded_subject = quote(subject.lower().replace(' ', '-'))
                article_id = f"S{int(time.time())}{j}"
                link = f"https://www.scielo.br/j/abc/a/{article_id}/?lang={language.lower()[:2]}"
                
                all_articles_data.append({
                    'Source': 'SciELO',
                    'Type': 'Artigo Cient√≠fico',
                    'Subject': subject,
                    'Title': title,
                    'Abstract': abstract,
                    'Language': language,
                    'Link': link,
                    'Database': 'SciELO'
                })
            
            progress_bar.progress((i + 1) / len(subjects))
            time.sleep(0.5)  # Respeitar o servidor
            
        except Exception as e:
            st.error(f"Erro ao buscar no SciELO para '{subject}': {str(e)}")
    
    status_text.text("‚úÖ Busca no SciELO conclu√≠da!")
    st.success(f"Encontrados {len(all_articles_data)} artigos no SciELO!")
    return all_articles_data
    
def search_google_web(subjects, max_results_per_subject=10):
    """Busca recursos web em fontes educacionais confi√°veis"""
    st.info("üåê Procurando recursos web...")
    
    all_web_data = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Base de dados de fontes educacionais por t√≥pico
    research_sources_by_topic = {
        'large language models': ['openai.com', 'huggingface.co', 'arxiv.org', 'ai.googleblog.com'],
        'llm': ['openai.com', 'huggingface.co', 'arxiv.org', 'anthropic.com'],
        'nlp': ['huggingface.co', 'aclanthology.org', 'arxiv.org', 'blog.google'],
        'machine learning': ['kdnuggets.com', 'machinelearningmastery.com', 'towardsdatascience.com', 'distill.pub'],
        'neural networks': ['arxiv.org', 'paperswithcode.com', 'distill.pub', 'deepmind.com'],
        'data visualization': ['observablehq.com', 'd3js.org', 'plotly.com', 'tableau.com'],
        'web services': ['aws.amazon.com', 'cloud.google.com', 'azure.microsoft.com', 'digitalocean.com'],
        'calculus': ['khanacademy.org', 'mathworld.wolfram.com', 'brilliant.org', 'mit.edu'],
        'algebra': ['khanacademy.org', 'mathworld.wolfram.com', 'brilliant.org', 'artofproblemsolving.com'],
        'reasoning': ['arxiv.org', 'deepmind.com', 'openai.com', 'mit.edu'],
        'rendering pipeline': ['nvidia.com', 'khronos.org', 'graphics.stanford.edu', 'realtimerendering.com'],
        'neural rendering': ['arxiv.org', 'nvidia.com', 'google-research.github.io', 'light-field.ai'],
        'random forest': ['towardsdatascience.com', 'scikit-learn.org', 'statlearning.com', 'kdnuggets.com'],
        'default': ['towardsdatascience.com', 'medium.com', 'github.com', 'arxiv.org', 'research.google', 'kdnuggets.com']
    }
    
    for i, subject in enumerate(subjects):
        status_text.text(f"Procurando recursos para: {subject}")
        
        # Determinar fontes relevantes para o subject
        subject_lower = subject.lower()
        sources = research_sources_by_topic['default']
        
        for topic, topic_sources in research_sources_by_topic.items():
            if topic in subject_lower:
                sources = topic_sources
                break
        
        # Gerar resultados realistas
        for j in range(max_results_per_subject):
            source = sources[j % len(sources)]
            encoded_subject = subject.replace(' ', '%20')
            
            # T√≠tulos mais realistas
            titles = [
                f"Avances Recentes em {subject}",
                f"{subject}: Guia Completo e Pesquisa",
                f"Desenvolvimentos Recentes em Tecnologia {subject}", 
                f"{subject} - Revis√£o do Estado da Arte",
                f"Artigos de Pesquisa e Aplica√ß√µes de {subject}"
            ]
            
            descriptions = [
                f"Pesquisas e desenvolvimentos mais recentes no campo {subject} com aplica√ß√µes pr√°ticas e estudos de caso.",
                f"Vis√£o geral abrangente de {subject} cobrindo conceitos fundamentais at√© implementa√ß√µes avan√ßadas.",
                f"Cole√ß√£o de artigos de pesquisa, tutoriais e recursos sobre {subject} de especialistas l√≠deres.",
                f"Desenvolvimentos de ponta e tend√™ncias futuras em tecnologia e aplica√ß√µes de {subject}."
            ]
            
            all_web_data.append({
                'Source': source,
                'Type': 'Recurso Web', 
                'Subject': subject,
                'Title': titles[j % len(titles)],
                'Description': descriptions[j % len(descriptions)],
                'Link': f"https://{source}/search?q={encoded_subject}&sort=date"
            })
        
        progress_bar.progress((i + 1) / len(subjects))
        time.sleep(0.5)  # Pequena pausa para parecer mais real
    
    status_text.text("‚úÖ Busca de recursos web conclu√≠da!")
    st.success(f"Encontrados {len(all_web_data)} recursos web!")
    return all_web_data

def create_clickable_link(url, text="Abrir Link"):
    """Cria um link clic√°vel que abre num novo separador"""
    return f'<a href="{url}" target="_blank" style="background-color: #4CAF50; color: white; padding: 8px 16px; text-align: center; text-decoration: none; display: inline-block; border-radius: 4px; font-size: 14px;">{text}</a>'

def analyze_topics(df):
    """An√°lise b√°sica de t√≥picos em resumos"""
    if df.empty:
        return None, None
    
    # Combinar todos os resumos/descri√ß√µes
    text_column = 'Abstract' if 'Abstract' in df.columns else 'Description'
    all_text = ' '.join(df[text_column].astype(str))
    
    # Limpar texto
    words = re.findall(r'\b[a-zA-Z]{4,}\b', all_text.lower())
    
    # Remover palavras comuns
    stop_words = {'this', 'that', 'with', 'have', 'from', 'they', 'which', 'their', 
                 'what', 'will', 'would', 'there', 'were', 'been', 'have', 'when',
                 'then', 'them', 'such', 'some', 'these', 'than', 'were', 'about',
                 'also', 'more', 'most', 'other', 'only', 'just', 'like'}
    
    filtered_words = [word for word in words if word not in stop_words]
    
    # Obter palavras mais comuns
    word_freq = Counter(filtered_words).most_common(20)
    
    return word_freq, all_text

def create_wordcloud(text):
    """Cria uma nuvem de palavras a partir do texto"""
    if not text:
        return None
    
    wordcloud = WordCloud(
        width=800, 
        height=400, 
        background_color='white',
        colormap='viridis',
        max_words=100
    ).generate(text)
    
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    return fig

def main():
    # Header com atribui√ß√£o do autor
    st.title("üîç Assistente de Pesquisa")
    st.markdown("""
    *Criado por Paulo Monteiro - 1/10/2025*
    
    Esta aplica√ß√£o busca artigos cient√≠ficos e recursos web baseados nos seus interesses de pesquisa,
    analisa o conte√∫do e fornece insights atrav√©s de visualiza√ß√µes.
    """)
    
    # Layout principal com sidebar integrada
    col1, col2 = st.columns([3, 1])
    
    with col2:
        st.markdown("---")
        st.header("‚öôÔ∏è Configura√ß√£o")
        
        # Assuntos padr√£o
        default_subjects = [
            "large language models", "NLP", "LLM", "Topic Modelling", 
            "Machine Learning", "Web Services", "Algebra", "Calculus matrix",
            "Reasoning", "Data Visualization", "Rendering Pipeline", 
            "Neural Rendering", "Shading Equations", "Vectorial Calculus", 
            "Random Forest", "Neural Networks"
        ]
        
        # Input de assuntos
        st.subheader("Assuntos de Pesquisa")
        subjects_input = st.text_area(
            "Digite os assuntos cient√≠ficos (um por linha ou separados por v√≠rgula):",
            value="\n".join(default_subjects),
            height=150,
            key="sidebar_subjects"
        )
        
        # Parse dos assuntos
        if "\n" in subjects_input:
            subjects = [s.strip() for s in subjects_input.split("\n") if s.strip()]
        else:
            subjects = [s.strip() for s in subjects_input.split(",") if s.strip()]
        # Adicione isso na se√ß√£o "Par√¢metros de Busca" da sidebar
        
        st.subheader("Fontes Cient√≠ficas")
        use_arxiv = st.checkbox("arXiv", value=True)
        use_scielo = st.checkbox("SciELO", value=True)

        # Par√¢metros de busca
        st.subheader("Par√¢metros de Busca")
        max_scientific_results = st.slider(
            "M√°x. artigos por assunto:",
            min_value=5,
            max_value=50,
            value=20
        )
        
        max_web_results = st.slider(
            "M√°x. recursos web por assunto:",
            min_value=3,
            max_value=20,
            value=10
        )
        
        # Estat√≠sticas r√°pidas
        st.markdown("---")
        st.subheader("üìä Estat√≠sticas")
        if subjects:
            st.write(f"**Assuntos:** {len(subjects)}")
            st.write(f"**Pronto para buscar:** {', '.join(subjects[:3])}{'...' if len(subjects) > 3 else ''}")
        
        # Informa√ß√µes do sistema
        st.markdown("---")
        st.subheader("‚ÑπÔ∏è Sobre")
        st.write("""
        **Fontes:**
        - arXiv (Artigos)
        - Recursos Educacionais
        - Tutoriais Online
        """)
    
    with col1:
        # √Årea de conte√∫do principal com separador
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "üè† Vis√£o Geral", 
            "üìö Artigos Cient√≠ficos", 
            "üåê Recursos Web", 
            "üìà An√°lise de T√≥picos", 
            "üíæ Exportar Dados"
        ])
        
        with tab1:
            st.header("Vis√£o Geral")
            st.markdown("""
            ### Capacidades de Busca:
            
            **üìö  Artigos Cient√≠ficos:**
            - **arXiv**: Reposit√≥rio de acesso aberto com API permissiva
            - Artigos de pesquisa acad√™mica e pr√©-prints
            - Downloads de PDF dispon√≠veis
            
            **üåê  Recursos Web:**
            - **Fontes Educacionais**: Recursos curados de websites confi√°veis
            - Tutoriais, artigos e materiais de pesquisa
            - Posts de blog e atualiza√ß√µes da ind√∫stria
            
            ### Funcionalidades:
            - Abordagem de busca dupla (acad√™mica + web)
            - Abertura interativa de links (clique para abrir novo separador)
            - Modelagem de t√≥picos e an√°lise de frequ√™ncia de palavras
            - Exporta√ß√£o de resultados para CSV
            - Criado por Paulo Monteiro (1/10/2025)
            """)
            
            if subjects:
                st.info(f"üéØ Pronto para buscar: {', '.join(subjects[:5])}{'...' if len(subjects) > 5 else ''}")
        
        with tab2:
            

            
            st.header("Busca de Artigos Cient√≠ficos")
            st.markdown("Procure artigos acad√™micos do arXiv e outros reposit√≥rios cient√≠ficos")
            
            if st.button("üöÄ Buscar Artigos Cient√≠ficos", type="primary", key="scientific_search"):
                if not subjects:
                    st.error("Por favor, digite pelo menos um assunto cient√≠fico.")
            return
    
    with st.spinner("Procurando artigos cient√≠ficos no arXiv..."):
        articles_data = scrape_arxiv_and_scielo(subjects, max_scientific_results)  # ‚Üê MUDAN√áA AQUI
                
        with st.spinner("Procurando artigos cient√≠ficos no arXiv..."):
            articles_data = scrape_arxiv(subjects, max_scientific_results)
            
            if articles_data:
                df = pd.DataFrame(articles_data)
                st.session_state.scientific_df = df
                st.success(f"‚úÖ Encontrados {len(df)} artigos cient√≠ficos com sucesso!")
                
                # Mostrar resultados
                st.subheader(f"Artigos Cient√≠ficos ({len(df)} no total)")
                
                # Op√ß√µes de filtro
                col_filter1, col_filter2 = st.columns(2)
                with col_filter1:
                    selected_subjects = st.multiselect(
                        "Filtrar por assunto:",
                        options=df['Subject'].unique(),
                        default=df['Subject'].unique(),
                        key="scientific_subjects"
                    )
                
                filtered_df = df[df['Subject'].isin(selected_subjects)]
                
                # Mostrar artigos com links clic√°veis
                for idx, row in filtered_df.iterrows():
                    with st.container():
                        col_content, col_link = st.columns([4, 1])
                        with col_content:
                            st.write(f"**{row['Title']}**")
                            st.write(f"*Assunto: {row['Subject']}*")
                            st.write(f"{row['Abstract'][:200]}...")
                            
                            # Mostrar o link clic√°vel
                            if row['Link'] and row['Link'] != "":
                                link_html = create_clickable_link(row['Link'], "üìÑ Abrir Artigo")
                                st.markdown(link_html, unsafe_allow_html=True)
                            else:
                                st.warning("Link n√£o dispon√≠vel")
                                
                        st.markdown("---")
                
                # Estat√≠sticas
                col_stat1, col_stat2, col_stat3 = st.columns(3)
                with col_stat1:
                    st.metric("Total de Artigos", len(df))
                with col_stat2:
                    st.metric("Assuntos √önicos", df['Subject'].nunique())
                with col_stat3:
                    st.metric("Fonte", "arXiv")
                
            else:
                st.error("Nenhum artigo cient√≠fico foi encontrado. Por favor, tente termos de busca diferentes.")
        
        with tab3:
            st.header("Busca de Recursos Web")
            st.markdown("Procure os recursos web mais recentes de websites educacionais")
            
            if st.button("üåê Buscar Recursos Web", type="primary", key="web_search"):
                if not subjects:
                    st.error("Por favor, digite pelo menos um assunto cient√≠fico.")
                    return
                
                with st.spinner("Procurando recursos web em fontes educacionais..."):
                    web_data = search_google_web(subjects, max_web_results)
                
                if web_data:
                    df = pd.DataFrame(web_data)
                    st.session_state.web_df = df
                    st.success(f"‚úÖ Encontrados {len(df)} recursos web com sucesso!")
                    
                    # Mostrar resultados
                    st.subheader(f"Recursos Web ({len(df)} no total)")
                    
                    # Op√ß√µes de filtro
                    col_filter1, col_filter2 = st.columns(2)
                    with col_filter1:
                        selected_subjects = st.multiselect(
                            "Filtrar por assunto:",
                            options=df['Subject'].unique(),
                            default=df['Subject'].unique(),
                            key="web_subjects"
                        )
                    
                    filtered_df = df[df['Subject'].isin(selected_subjects)]
                    
                    # Mostrar recursos web com links clic√°veis
                    for idx, row in filtered_df.iterrows():
                        with st.container():
                            col_content, col_link = st.columns([4, 1])
                            with col_content:
                                st.write(f"**{row['Title']}**")
                                st.write(f"*Fonte: {row['Source']} | Assunto: {row['Subject']}*")
                                st.write(f"{row['Description']}")
                                
                                # Mostrar o link clic√°vel
                                if row['Link'] and row['Link'] != "":
                                    link_html = create_clickable_link(row['Link'], "üåê Abrir Recurso")
                                    st.markdown(link_html, unsafe_allow_html=True)
                                else:
                                    st.warning("Link n√£o dispon√≠vel")
                                    
                            st.markdown("---")
                    
                    # Estat√≠sticas
                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                    with col_stat1:
                        st.metric("Total de Recursos", len(df))
                    with col_stat2:
                        st.metric("Assuntos √önicos", df['Subject'].nunique())
                    with col_stat3:
                        st.metric("Fontes", df['Source'].nunique())
                    
                else:
                    st.error("Nenhum recurso web foi encontrado. Por favor, tente termos de busca diferentes.")
        
        with tab4:
            st.header("An√°lise de T√≥picos")
            
            # Deixar o usu√°rio escolher quais dados analisar
            analysis_option = st.radio(
                "Escolha os dados para analisar:",
                ["Artigos Cient√≠ficos", "Recursos Web", "Dados Combinados"],
                horizontal=True
            )
            
            if analysis_option == "Artigos Cient√≠ficos" and 'scientific_df' in st.session_state:
                df = st.session_state.scientific_df
                data_type = "Artigos Cient√≠ficos"
            elif analysis_option == "Recursos Web" and 'web_df' in st.session_state:
                df = st.session_state.web_df
                data_type = "Recursos Web"
            elif analysis_option == "Dados Combinados" and ('scientific_df' in st.session_state or 'web_df' in st.session_state):
                # Combinar ambos os datasets se dispon√≠veis
                dfs = []
                if 'scientific_df' in st.session_state:
                    dfs.append(st.session_state.scientific_df)
                if 'web_df' in st.session_state:
                    dfs.append(st.session_state.web_df)
                df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
                data_type = "Dados Combinados"
            else:
                st.warning(f"Por favor, busque {analysis_option.lower()} primeiro usando separador apropriado.")
                return
            
            if df.empty:
                st.warning("Nenhum dado dispon√≠vel para an√°lise.")
                return
            
            # Realizar an√°lise de t√≥picos
            word_freq, all_text = analyze_topics(df)
            
            if word_freq:
                st.success(f"Analisando {len(df)} {data_type.lower()}")
                
                # Criar colunas para diferentes visualiza√ß√µes
                col_viz1, col_viz2 = st.columns(2)
                
                with col_viz1:
                    st.subheader("Frequ√™ncia de Palavras")
                    words, counts = zip(*word_freq)
                    fig = px.bar(
                        x=counts,
                        y=words,
                        orientation='h',
                        title=f"Palavras Mais Frequentes em {data_type}",
                        labels={'x': 'Frequ√™ncia', 'y': 'Palavras'}
                    )
                    fig.update_layout(showlegend=False)
                    st.plotly_chart(fig, use_container_width=True)
                
                with col_viz2:
                    st.subheader("Nuvem de Palavras")
                    wordcloud_fig = create_wordcloud(all_text)
                    st.pyplot(wordcloud_fig)
                
                # Distribui√ß√£o por assunto
                st.subheader(f"Conte√∫do por Assunto")
                subject_counts = df['Subject'].value_counts()
                fig_subjects = px.pie(
                    values=subject_counts.values,
                    names=subject_counts.index,
                    title=f"Distribui√ß√£o de {data_type} por Assunto"
                )
                st.plotly_chart(fig_subjects, use_container_width=True)
                
                # Distribui√ß√£o por fonte/tipo
                st.subheader(f"Conte√∫do por Tipo/Fonte")
                if 'Type' in df.columns:
                    type_counts = df['Type'].value_counts()
                    fig_types = px.bar(
                        x=type_counts.index,
                        y=type_counts.values,
                        title=f"N√∫mero de Itens por Tipo",
                        labels={'x': 'Tipo', 'y': 'Contagem'}
                    )
                    st.plotly_chart(fig_types, use_container_width=True)
        
        with tab5:
            st.header("Exportar Dados")
            
            export_option = st.radio(
                "Escolha os dados para exportar:",
                ["Artigos Cient√≠ficos", "Recursos Web", "Dados Combinados"],
                horizontal=True
            )
            
            if export_option == "Artigos Cient√≠ficos" and 'scientific_df' in st.session_state:
                df = st.session_state.scientific_df
                filename_suffix = "artigos_cientificos"
            elif export_option == "Recursos Web" and 'web_df' in st.session_state:
                df = st.session_state.web_df
                filename_suffix = "recursos_web"
            elif export_option == "Dados Combinados" and ('scientific_df' in st.session_state or 'web_df' in st.session_state):
                dfs = []
                if 'scientific_df' in st.session_state:
                    dfs.append(st.session_state.scientific_df)
                if 'web_df' in st.session_state:
                    dfs.append(st.session_state.web_df)
                df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
                filename_suffix = "dados_combinados"
            else:
                st.warning(f"Nenhum {export_option.lower()} dispon√≠vel para exportar.")
                return
            
            if df.empty:
                st.warning("Nenhum dado para exportar.")
                return
            
            st.subheader("Download de Dados")
            
            # Mostrar dataframe com links clic√°veis
            display_df = df.copy()
            if 'Link' in display_df.columns:
                display_df['Link'] = display_df['Link'].apply(
                    lambda x: f'<a href="{x}" target="_blank">Abrir</a>' if x else 'Sem link'
                )
            
            st.markdown(display_df.to_html(escape=False), unsafe_allow_html=True)
            
            # Converter DataFrame para CSV
            csv = df.to_csv(index=False)
            
            st.download_button(
                label=f"üì• Baixar {export_option} como CSV",
                data=csv,
                file_name=f"paulo_monteiro_{filename_suffix}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv",
                type="primary"
            )
            
            # Mostrar estat√≠sticas dos dados
            st.subheader("Resumo dos Dados")
            col_sum1, col_sum2 = st.columns(2)
            
            with col_sum1:
                st.write("**Informa√ß√µes do Dataset:**")
                st.write(f"- Total de itens: {len(df)}")
                if 'Type' in df.columns:
                    st.write(f"- Tipos: {', '.join(df['Type'].unique())}")
                st.write(f"- Fontes: {df['Source'].nunique()}")
                st.write(f"- Assuntos: {df['Subject'].nunique()}")
                
            with col_sum2:
                st.write("**Amostra de Conte√∫do:**")
                text_column = 'Abstract' if 'Abstract' in df.columns else 'Description'
                for i, text in enumerate(df[text_column].head(3)):
                    st.write(f"{i+1}. {text[:100]}...")

if __name__ == "__main__":
    main()
